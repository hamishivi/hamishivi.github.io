---
layout: post
title: Does GPT-3 know Ancient Greek?
tags: blog non-technical technical
description: Poking around with gpt-3 and ancient languages
---

I don't know quite why, but recently I've been thinking about zero-shot multilingual translation, and so I decided to try to see how well GPT-3 can handle low-resource ancient languages. I have some training in Latin and Ancient Greek (technically, I have a degree in Ancient Greek), so I figured I would see how well it went with them. This is going to just be a *qualitative* analysis, rather than me trying to build some Latin/Ancient Greek translation benchmark, to keep things light and easy. By no means is this thorough or definitive!

## Latin

Latin has more text extant than Ancient Greek (I think - at least, it would be more common than Ancient Greek in some random web/book crawl). So this should mean GPT-3 is better at it. Furthermore, Google Translate Latin exists, which provides a fun comparison.

Let's take a few lines from Book 12 of the Aeneid, and see how it goes. For those who haven't used GPT-3 before, it works by *prompting* it with some text in natural language, and then letting it autocomplete your text (a bit like pressing autocomplete on a keyboard, but for a full paragraph). People often try out multiple prompts to work out what is best - I messed around a little and settled on a basic format of `Translate the following to English: <text> English:`, which seemed to do okay. In the following chunks, **bolded** indicates model-generated text, with the rest of the text being input by me:


>Translate the following to English:
>
>Turnus ut infractos adverso Marte Latinos defecisse videt, sua nunc promissa reposci, se signari oculis, ultro implacabilis ardet attollitque animos. Poenorum qualis in arvis saucius ille gravi venantum vulnere pectus tum demum movet arma leo gaudetque comantis excutiens cervice toros fixumque latronis inpavidus frangit telum et fremit ore cruento: haud secus adcenso gliscit violentia Turno.
>
>English:
>
>**Turnus sees the Latins defeated by Mars and is furious, demanding that his promises be fulfilled. He is determined to take revenge and rallies his troops. Just as a wounded lion in the fields finally moves to attack the hunters who wounded him, after shaking off the blood from his mane, and breaks the spear of the bold robber with his powerful neck, so too does Turnus' fury grow as he climbs up to fight.**

For those of you who didn't spend your final year of high school memorising book 12 of the Aeneid, here's a professional translation ([from poetry in translation](https://www.poetryintranslation.com/PITBR/Latin/VirgilAeneidXII.php#anchor_Toc6669703)):
> When Turnus saw the Latins exhausted, and weakened by their military reverse, himself the subject of every gaze, his own promise to them yet unfulfilled, he burned implacably, and unprompted, and raised his courage. As a lion, in the African bush, severely hurt by huntsmen with a wound to the chest, only then rouses himself to battle, tosses his shaggy mane over his neck, in joy, and, unafraid, snaps off the spear some poacher has planted in him, roaring from blood-stained jaws: so the violence grew in Turnus’s inflamed heart.

This does pretty well! It sort of muddles bits, but the lion simile remains intact in a nice way. By the way, Google Translate does awful here:
> Turnus sees that the Latins, broken in opposition to Mars, have failed, now that their promises have been restored, and that they have been sealed with their eyes; He inflicts punishments such as those in the fields on the chest of the hunted with a severe wound, then at last he moves his arms like a lion and rejoices, shaking the neck of the commander, fixed on his neck, and fearlessly breaks his weapon and roars with a bloody mouth: just as the burning violence of Turnus glides.

So it appears (to me) that GPT-3 has a better idea of Latin than whatever Google uses! The Aeneid is too easy, though - an all-time classic, which has probably appeared all over in GPT-3's training. Let's try something a bit different, Ovid's *Ars Amatoria* (still famous, but not as mega-famous as *the* Roman Epic). I'll use an easier sentence:

>Translate the following to English:
>Atque aliqua ex illis cum regum nomina quaeret, Quae loca, qui montes, quaeve ferantur aquae, Omnia responde, nec tantum siqua rogabit; Et quae nescieris, ut bene nota refer.
>
>English:
>
>**And when she asks about the names of the kings, the places, the mountains, and the waters that are mentioned, answer everything, and even if she doesn't ask, explain what you know clearly. And if you don't know something, say so.**

Here's Google's version:
> And when some of them ask for the names of kings, what places, what mountains, what waters are carried, answer everything, and he will not ask so much; And what you do not know, report as well known.

And a more professional, actual translation:
> And if she, among them, asks the name of a king, what place, what mountains, and what stream’s displayed, you can reply to all, and more if she asks: and what you don’t know, reply as memory prompts.

So GPT-3 did pretty well here! Although the final sentence is wrong (Ovid says to say what you can recall, rather than to say if you don't know).

For good measure, let's try something legal (albeit still fairly straightforward): Cicero's *Pro Roscio Amerino*, a famous legal defence.  

>Translate the following to English:
>fide sapientiaque vestra fretus plus oneris sustuli quam ferre me posse intellego. hoc onus si vos aliqua ex parte adlevabitis, feram ut potero studio et industria, iudices; sin a vobis, id quod non spero, deserar, tamen animo non deficiam et id quod suscepi quoad potero perferam. quod si perferre non potero1, opprimi me onere offici malo quam id quod mihi cum fide semel impositum est aut propter perfidiam abicere aut propter infirmitatem animi deponere.
>
>English:
>
>**I realize that I have taken on more than I can handle, but I am relying on your wisdom and support. If you can help me in any way, I will do my best to carry on, judges. If I am abandoned by you, which I do not hope for, I will not give up and will do my best to carry out what I have started. If I cannot do so, I would rather be crushed by the weight of my responsibilities than abandon or give up on what has been entrusted to me.**

A professional translation would be:
> Relying on your integrity and wisdom, I have undertaken a greater burden than, I am well aware, I am able to bear. If you, in some degree, lighten this burden, O judges, I will bear it as well as I can with zeal and industry. But if, as I do not expect, I am abandoned by you, still I will not fail in courage, and I will bear what I have undertaken as well as I can. But if I cannot support it, I had rather be overwhelmed by the weight of my duty, than either through treachery betray, or through weakness of mind desert, that which has been once honestly entrusted to me.

As a final fun test, let's see if GPT-3 knows the basic forms of the verb to be:
> What is the 1st person singular form of the verb 'to be' in Latin?
> **Sum**

>What is the 2nd person singular form of the verb 'to be' in Latin?
>**The 2nd person singular form of the verb 'to be' in Latin is 'est'.**

>What is the 3rd person singular form of the verb 'to be' in Latin?
>**The third person singular form of the verb "to be" in Latin is "est."**

>What is the 1st person plural form of the verb 'to be' in Latin?
>**sumus**

>What is the 2nd person plural form of the verb 'to be' in Latin?
>**You are (plural)**

>What is the 3rd person plural form of the verb 'to be' in Latin?
>**Sunt.**

Well, it gets a bit confused, although I think this is quite hard considering the prompt doesn't give any examples. I find it interesting it gets the 2nd person forms wrong, considering those are forms I have found to be far far less common than the others.

This did a really good job! Overall, I'm really impressed by GPT-3's Latin abilities. I wouldn't put it past it having memorised these texts (these all exist on the internet with translations usually somewhere in the same document), but it's still quite impressive to see. It also handily beats Google, which is fun. 


## Ancient Greek

Let's start with something really easy: the first four lines of the *Iliad*. 

>Translate the following to English:
>
> μῆνιν ἄειδε θεὰ Πηληϊάδεω Ἀχιλῆος οὐλομένην, ἣ μυρί᾽ Ἀχαιοῖς ἄλγε᾽ ἔθηκε, πολλὰς δ᾽ ἰφθίμους ψυχὰς Ἄϊδι προΐαψεν ἡρώων, αὐτοὺς δὲ ἑλώρια τεῦχε κύνεσσιν οἰωνοῖσί τε πᾶσι, Διὸς δ᾽ ἐτελείετο βουλή, ἐξ οὗ δὴ τὰ πρῶτα διαστήτην ἐρίσαντε  Ἀτρεΐδης τε ἄναξ ἀνδρῶν καὶ δῖος Ἀχιλλεύς.
>
> English:
> **Achilles, the son of Peleus, killed Hector, who had caused great harm to the Achaeans. He also killed many other brave men, and he terrified those who were still alive. This was the will of Zeus, who had first stirred up the conflict between King Atreus' son Agamemnon and noble Achilles.**

This is already worse - Hector doesn't even appear in the Greek text! However, overall it's okay, seemingly getting a sense of the text, although being definitely incorrect at times (unlike the Latin translations, which tended to be correct). For those of you who don't know the Iliad by heart, these lines would be translated something like (from [poetry in translation](https://www.poetryintranslation.com/PITBR/Greek/Iliad1.php)):

> Goddess, sing me the anger, of Achilles, Peleus’ son,
> that fatal anger that brought countless sorrows on the Greeks,
> and sent many valiant souls of warriors down to Hades,
> leaving their bodies as spoil for dogs and
> carrion birds: for thus was the will of Zeus brought to fulfilment.
> Sing of it from the moment when Agamemnon, Atreus’ son, that king of men, 
> parted in wrath from noble Achilles.

If you prompt it with the first clause(s), it gets a much better translation, likely due to it having encountered the beginning of the Iliad countless times in training:

>Sing, Muse, of the wrath of Achilles, son of Peleus, **that brought countless ills upon the Achaeans. Many a brave soul did it send hurrying down to Hades, and many a hero did it yield a prey to dogs and vultures, for so was the will of Zeus fulfilled from the time when first they quarreled,Atreus’ son, king of men, and brilliant Achilles.**

There are many reasons GPT-3 could doing worse for Greek, but one large change from Latin is the use of a different script. Let's try normalising the text into beta code (a standard conversion for Ancient Greek into ASCII) and see how it goes:

> Translate the following to English:
>
> mh=nin a)/eide qea\ *phlhi+a/dew *)axilh=os ou)lome/nhn, h(\ muri/' *)axaioi=s a)/lge' e)/qhke, polla\s d' i)fqi/mous yuxa\s *)/ai+di proi/+ayen h(rw/wn, au)tou\s de\ e(lw/ria teu=xe ku/nessin oi)wnoi=si/ te pa=si, *dio\s d' e)telei/eto boulh/, e)c ou(= dh\ ta\ prw=ta diasth/thn e)ri/sante  *)atrei/+dhs te a)/nac a)ndrw=n kai\ di=os *)axilleu/s.
>
> English:
>
> **After he had seen the many wrongs that had been done to Axylus, a just man, Hercules went to the land of the Achaeans and killed many of the evil men there. But he spared the life of Axylus, because he was a good man. So Zeus gave him a city and made him a god, and the first temple to Hercules was built by the men of Troy after they had been saved from disaster by Hercules and by Zeus himself.**

This is even worse! Removing the accent symbols (leaving only the letters) also doesn't help. Evidently, GPT-3 doesn't really know Ancient Greek at all (although it has definitely seen the Iliad).

Since the Iliad is so famous, let's try something more obscure. Here's a paragraph from Heron's *On Automata*:

>Translate the following to English:
>
>καθὰ δὲ προεθέμην, ἐρῶ περὶ ἑνὸς πίνακος τοῦ δοκοῦντός μοι κρείττονος. μῦθος μὲν ἦν τεταγμένος ἐν αὐτῷ ὁ κατὰ τὸν Ναύπλιον. τὰ δὲ κατὰ μέρος εἶχεν οὕτως· ἀνοιχθέντος ἐν ἀρχῇ τοῦ πίνακος ἐφαίνετο ζῴδια γεγραμμένα δώδεκα· ταῦτα δὲ ἦν εἰς τρεῖς στίχους διῃρημένα· ἦσαν δὲ οὗτοι πεποιημένοι τῶν Δαναῶν τινες ἐπισκευάζοντες τὰς ναῦς καὶ γινόμενοι περὶ καθολκήν.
>
>English:
>**I was once thinking about a picture that seemed better to me than it actually was. The subject of the picture was the story of the Odyssey. Part of the picture showed the twelve Labors of Hercules, which were divided into three groups. These were done by some of the Danaids who were repairing their ships and were becoming burdened.**

And if I prompt the model with the first few words:

> ...English:
> As I laid out before, **I was speaking about a painting that seemed better to me. The painting was hung in such a way that in the beginning, when you opened it, you saw twelve animals painted on it, divided into three verses. These were some of the Danaans, who, when they were repairing their ships, became encircled.**

Here's the correct translation:

>As I laid out before, I will talk about one box that seems superior to me. The story set in it was the one about Naupilius. And its parts went like this. In the beginning, when the box opened, 12 painted figurines appeared. These were divided into 3 rows; and these were made to represent some of the Danaans (Greeks) preparing their ships and launching them.


So, the model has clearly learnt some vocab - it gets the number 12, it works out Danaans, ships, and a few other things. These translations seem vaguely familiar but ultimately are definitely incorrect. It's cool that it works this well, but future classics scholars are definitely not going to be able to use it to cheat on their university homework 😜

## Wrapping this up

Unsurprisingly, GPT-3 is pretty good! It's Latin translation abilities seem pretty good, and it definitely feels like it has seen ancient texts written in English enough to regurgitate them when prompted. Its success at Latin makes its harsher failures in translating Ancient Greek a little surprising, but I suspect the big switch into a different script makes things much harder, and that there is simply far less Ancient Greek text in its pretraining corpus. I've focussed on languages I'm personally familiar with here, but it would also be interesting to see if one could somehow get a large language model like GPT-3 to give its best guess on how to translate Linear A or similar!

As a technical note, I used `text-davinci-002` for this post.