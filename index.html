<!DOCTYPE html>
<html lang="en"><head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Hamish Ivison | Hamish’s site.</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Hamish Ivison" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Hamish’s site." />
<meta property="og:description" content="Hamish’s site." />
<link rel="canonical" href="https://hamishivi.github.io/" />
<meta property="og:url" content="https://hamishivi.github.io/" />
<meta property="og:site_name" content="Hamish Ivison" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Hamish Ivison" />
<script type="application/ld+json">
{"description":"Hamish’s site.","url":"https://hamishivi.github.io/","@type":"WebSite","headline":"Hamish Ivison","name":"Hamish Ivison","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

      





    
    <link rel="stylesheet" href="/assets/css/style_dark.css" >
    <link rel="stylesheet" href="/assets/css/style_dark.css" media="(prefers-color-scheme: dark)">
    <link rel="stylesheet" href="/assets/css/style_light.css" media="(prefers-color-scheme: light)"><link type="application/atom+xml" rel="alternate" href="https://hamishivi.github.io/feed.xml" title="Hamish Ivison" />

<script type="module" src="https://unpkg.com/dark-mode-toggle"></script>
<script src="https://code.jquery.com/jquery-3.6.0.slim.min.js" integrity="sha256-u7e5khyithlIdTpu22PHhENmPcRdFiHRjhAuHcs05RI=" crossorigin="anonymous"></script>
<script>
  $(document).ready(function(){
      $(".btnId").click(function(){
          var str = $(this).attr('id');
          var ret = str.split("_");
          var id = ret[1];
          $('#' + id).toggle();
          $(this).toggleClass("dropped");
      });
  });

</script>

    
  </head><body><header class="site-header">

    <div class="wrapper"><a class="site-title" rel="author" href="/"><span style="color: #2ecc71">H</span>amish <span style="color: #2ecc71">I</span>vison</a><nav class="site-nav">
          <input type="checkbox" id="nav-trigger" class="nav-trigger" />
          <label for="nav-trigger">
            <span class="menu-icon">
              <svg viewBox="0 0 18 15" width="18px" height="15px">
                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
              </svg>
            </span>
          </label>
  
          <div class="trigger"><a class="page-link" href="/">About</a><a class="page-link" href="/non-technical.html">Random</a><a class="page-link" href="/technical.html">Side Projects</a></div>
        </nav></div>
    <dark-mode-toggle
    id="theme-toggle"
    appearance="toggle"
    style="position: absolute; right: 25px; top: 60%;"></dark-mode-toggle>
  </header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="home"><img src="/assets/static/me.jpg" alt="A photo of me!" style="width:25%; border-radius: 50%; float: left; margin: 1em">

  <p>Hi, I’m <a href="#" data-tooltip="[ˈheɪmɪʃ]">Hamish</a>! I’m (currently) a PhD student at the University of Washington at <a href="https://h2lab.cs.washington.edu/">H2Lab</a>, advised by Hannaneh Hajishirzi. I’m generally interested in NLP research, with interests in making language models more easy to use and open, exploring alternative architectures, and linking model abilities and data.</p>

<p>I’m originally from Sydney, and did my undergraduate at the University of Sydney with a Bachelor of Arts and IT, triple majoring in Linguistics, Classical Greek, and Computer Science. I also did some NLP with the <a href="https://usydnlp.info/">UsydNLP group</a>, examining multi-hop question answering. Throughout my undergrad (and just after), I spent some time at the <a href="https://www.commbank.com.au/">Commonwealth Bank of Australia</a>, start-up-y stuff, and <a href="https://www.optiver.com/">Optiver</a>. Before my PhD, I was a predoctoral researcher at <a href="https://allenai.org/">AI2</a> on the <a href="https://allenai.org/allennlp">AllenNLP team</a>.</p>

<p>If you have questions about my work, general academia/software/research-related stuff, or want to chat, feel free to reach out at hamishiv [at] cs [dot] washington [dot] edu. I’m generally down to chat about whatever!</p>

<hr />

<h2>Papers</h2>

<p>See below for papers I’ve worked on. You can also check out my <a href="https://www.semanticscholar.org/author/Hamish-Ivison/2056776606">Semantic Scholar</a> and <a href="https://scholar.google.com/citations?user=JxCXMlkAAAAJ">Google Scholar</a> profiles.</p>

<div class="bibliography"><ul><span id="ivison2023camels"><b>Hamish Ivison*</b>, Yizhong Wang*, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. 2023. Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2.</span>
<br />
    
    <button class="btn btnId btnPub--abstract button-4" id="b_ivison2023camels-abstract" style="outline:none;">Abstract</button>
    

    <button class="btn btnId btnPub--BibTex button-4" id="b_ivison2023camels-bibtex" style="outline:none;">BibTeX</button>
    
    
    <a href="https://arxiv.org/abs/2311.10702"><button class="btn button-4 btnId btnPub--download" style="outline:none; position:relative;white-space: normal;">PDF</button></a>
    
    
    

    <div class="dropDownBibtex" id="ivison2023camels-bibtex" style="display:none;">
    <pre>@misc{ivison2023camels,
  title = {Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2},
  author = {<b>Hamish Ivison*</b> and Wang*, Yizhong and Pyatkin, Valentina and Lambert, Nathan and Peters, Matthew and Dasigi, Pradeep and Jang, Joel and Wadden, David and Smith, Noah A. and Beltagy, Iz and Hajishirzi, Hannaneh},
  year = {2023},
  url = {https://arxiv.org/abs/2311.10702},
  eprint = {2311.10702},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL}
}
</pre>
    </div>
    
    <div class="dropDownAbstract" id="ivison2023camels-abstract" style="display:none;">
    Since the release of TÜLU [Wang et al., 2023b], open resources for instruction tuning have developed quickly, from better base models to new finetuning techniques. We test and incorporate a number of these advances into TÜLU, resulting in TÜLU 2, a suite of improved TÜLU models for advancing the understanding and best practices of adapting pretrained language models to downstream tasks and user preferences. Concretely, we release: (1) TÜLU-V2-mix, an improved collection of high-quality instruction datasets; (2) TÜLU 2, LLAMA-2 models finetuned on the V2 mixture; (3) TÜLU 2+DPO, TÜLU 2 models trained with direct preference optimization (DPO), including the largest DPO-trained model to date (TÜLU 2+DPO 70B); (4) CODE TÜLU 2, CODE LLAMA models finetuned on our V2 mix that outperform CODE LLAMA and its instruction-tuned variant, CODE LLAMA-Instruct. Our evaluation from multiple perspectives shows that the TÜLU 2 suite achieves state-of-the-art performance among open models and matches or exceeds the performance of GPT-3.5-turbo-0301 on several benchmarks. We release all the checkpoints, data, training and evaluation code to facilitate future open efforts on adapting large language models.
    </div></ul>
<ul><span id="tulu">Yizhong Wang*, <b>Hamish Ivison*</b>, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. 2023. How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources.</span>
<br />
    
    <button class="btn btnId btnPub--abstract button-4" id="b_tulu-abstract" style="outline:none;">Abstract</button>
    

    <button class="btn btnId btnPub--BibTex button-4" id="b_tulu-bibtex" style="outline:none;">BibTeX</button>
    
    
    <a href="https://arxiv.org/abs/2306.04751"><button class="btn button-4 btnId btnPub--download" style="outline:none; position:relative;white-space: normal;">PDF</button></a>
    
    
    

    <div class="dropDownBibtex" id="tulu-bibtex" style="display:none;">
    <pre>@misc{tulu,
  title = {How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources},
  author = {Wang*, Yizhong and <b>Hamish Ivison*</b> and Dasigi, Pradeep and Hessel, Jack and Khot, Tushar and Chandu, Khyathi Raghavi and Wadden, David and MacMillan, Kelsey and Smith, Noah A. and Beltagy, Iz and Hajishirzi, Hannaneh},
  year = {2023},
  url = {https://arxiv.org/abs/2306.04751},
  eprint = {2306.04751},
  journal = {NeurIPS Datasets and Benchmarks Track},
  primaryclass = {cs.CL}
}
</pre>
    </div>
    
    <div class="dropDownAbstract" id="tulu-abstract" style="display:none;">
    In this work we explore recent advances in instruction-tuning language models on a range of open instruction-following datasets. Despite recent claims that open models can be on par with state-of-the-art proprietary models, these claims are often accompanied by limited evaluation, making it difficult to compare models across the board and determine the utility of various resources. We provide a large set of instruction-tuned models from 6.7B to 65B parameters in size, trained on 12 instruction datasets ranging from manually curated (e.g., OpenAssistant) to synthetic and distilled (e.g., Alpaca) and systematically evaluate them on their factual knowledge, reasoning, multilinguality, coding, and open-ended instruction following abilities through a collection of automatic, model-based, and human-based metrics. We further introduce Tülu, our best performing instruction-tuned model suite finetuned on a combination of high-quality open resources. Our experiments show that different instruction-tuning datasets can uncover or enhance specific skills, while no single dataset (or combination) provides the best performance across all evaluations. Interestingly, we find that model and human preference-based evaluations fail to reflect differences in model capabilities exposed by benchmark-based evaluations, suggesting the need for the type of systemic evaluation performed in this work. Our evaluations show that the best model in any given evaluation reaches on average 83% of ChatGPT performance, and 68% of GPT-4 performance, suggesting that further investment in building better base models and instruction-tuning data is required to close the gap. We release our instruction-tuned models, including a fully finetuned 65B Tülu, along with our code, data, and evaluation framework at https://github.com/allenai/open-instruct to facilitate future research.
    </div></ul>
<ul><span id="tess">Rabeeh Karimi Mahabadi, Jaesung Tae, <b>Hamish Ivison</b>, James Henderson, Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2023. TESS: Text-to-Text Self-Conditioned Simplex Diffusion. <i>arxiv</i>.</span>
<br />
    
    <button class="btn btnId btnPub--abstract button-4" id="b_tess-abstract" style="outline:none;">Abstract</button>
    

    <button class="btn btnId btnPub--BibTex button-4" id="b_tess-bibtex" style="outline:none;">BibTeX</button>
    
    
    <a href="https://arxiv.org/abs/2305.08379"><button class="btn button-4 btnId btnPub--download" style="outline:none; position:relative;white-space: normal;">PDF</button></a>
    
    
    

    <div class="dropDownBibtex" id="tess-bibtex" style="display:none;">
    <pre>@article{tess,
  author = {Mahabadi, Rabeeh Karimi and Tae, Jaesung and <b>Hamish Ivison</b> and Henderson, James and Beltagy, Iz and Peters, Matthew E. and Cohan, Arman},
  title = {TESS: Text-to-Text Self-Conditioned Simplex Diffusion},
  journal = {arxiv},
  url = {https://arxiv.org/abs/2305.08379},
  year = {2023}
}
</pre>
    </div>
    
    <div class="dropDownAbstract" id="tess-abstract" style="display:none;">
    Diffusion models have emerged as a powerful paradigm for generation, obtaining strong performance in various domains with continuous-valued inputs. Despite the promises of fully non-autoregressive text generation, applying diffusion models to natural language remains challenging due to its discrete nature. In this work, we propose Text-to-text Self-conditioned Simplex Diffusion (TESS), a text diffusion model that is fully non-autoregressive, employs a new form of self-conditioning, and applies the diffusion process on the logit simplex space rather than the typical learned embedding space. Through extensive experiments on natural language understanding and generation tasks including summarization, text simplification, paraphrase generation, and question generation, we demonstrate that TESS outperforms state-of-the-art non-autoregressive models and is competitive with pretrained autoregressive sequence-to-sequence models.
    </div></ul>
<ul><span id="hint"><b>Hamish Ivison</b>, Akshita Bhagia, Yizhong Wang, Hannaneh Hajishirzi, and Matthew Peters. 2023. HINT: Hypernetwork Instruction Tuning for Efficient Zero-Shot Generalisation. <i>ACL</i>.</span>
<br />
    
    <button class="btn btnId btnPub--abstract button-4" id="b_hint-abstract" style="outline:none;">Abstract</button>
    

    <button class="btn btnId btnPub--BibTex button-4" id="b_hint-bibtex" style="outline:none;">BibTeX</button>
    
    
    <a href="https://arxiv.org/abs/2212.10315"><button class="btn button-4 btnId btnPub--download" style="outline:none; position:relative;white-space: normal;">PDF</button></a>
    
    
    

    <div class="dropDownBibtex" id="hint-bibtex" style="display:none;">
    <pre>@article{hint,
  author = {<b>Hamish Ivison</b> and Bhagia, Akshita and Wang, Yizhong and Hajishirzi, Hannaneh and Peters, Matthew},
  title = {HINT: Hypernetwork Instruction Tuning for Efficient Zero-Shot Generalisation},
  journal = {ACL},
  url = {https://arxiv.org/abs/2212.10315},
  year = {2023}
}
</pre>
    </div>
    
    <div class="dropDownAbstract" id="hint-abstract" style="display:none;">
    Recent NLP models have the great ability to generalise ‘zero-shot’ to new tasks using only an instruction as guidance. However, these approaches usually repeat their instructions with every input, requiring costly reprocessing of lengthy instructions for every inference example. To alleviate this, we introduce Hypernetworks for INstruction Tuning (HINT), which convert task instructions and examples using a pretrained text encoder into parameter-efficient modules inserted into an underlying model, eliminating the need to include instructions in the model input. Compared to prior approaches that concatenate instructions with every input instance, we find that HINT models are significantly more compute-efficient and consistently outperform these approaches for a given inference budget.
    </div></ul>
<ul><span id="deft"><b>Hamish Ivison</b>, Noah A. Smith, Hannaneh Hajishirzi, and Pradeep Dasigi. 2023. Data-Efficient Finetuning Using Cross-Task Nearest Neighbors. <i>Findings of ACL</i>.</span>
<br />
    
    <button class="btn btnId btnPub--abstract button-4" id="b_deft-abstract" style="outline:none;">Abstract</button>
    

    <button class="btn btnId btnPub--BibTex button-4" id="b_deft-bibtex" style="outline:none;">BibTeX</button>
    
    
    <a href="https://arxiv.org/abs/2212.00196"><button class="btn button-4 btnId btnPub--download" style="outline:none; position:relative;white-space: normal;">PDF</button></a>
    
    
    

    <div class="dropDownBibtex" id="deft-bibtex" style="display:none;">
    <pre>@article{deft,
  author = {<b>Hamish Ivison</b> and Smith, Noah A. and Hajishirzi, Hannaneh and Dasigi, Pradeep},
  title = {Data-Efficient Finetuning Using Cross-Task Nearest Neighbors},
  journal = {Findings of ACL},
  url = {https://arxiv.org/abs/2212.00196},
  year = {2023}
}
</pre>
    </div>
    
    <div class="dropDownAbstract" id="deft-abstract" style="display:none;">
    Language models trained on massive prompted multitask datasets like T0 (Sanh et al., 2021) or FLAN (Wei et al., 2021a) can generalize to tasks unseen during training. We show that training on a carefully chosen subset of instances can outperform training on all available data on a variety of datasets. We assume access to a small number (250–1000) of unlabeled target task instances, select their nearest neighbors from a pool of multitask data, and use the retrieved data to train target task-specific models. Our method is more data-efficient than training a single multitask model, while still outperforming it by large margins. We evaluate across a diverse set of tasks not in the multitask pool we retrieve from, including those used to evaluate T0 and additional complex tasks including legal and scientific document QA. We retrieve small subsets of P3 (the collection of prompted datasets from which T0’s training data was sampled) and finetune T5 models that outperform the 3-billion parameter variant of T0 (T0-3B) by 3–30% on 12 out of 14 evaluation datasets while using at most 2% of the data used to train T0-3B. These models also provide a better initialization than T0-3B for few-shot finetuning on target-task data, as shown by a 2–23% relative improvement over few-shot finetuned T0-3B models on 8 datasets. Our code is available at https://github.com/allenai/data-efficient-finetuning.
    </div></ul>
<ul><span id="hyperdecoders"><b>Hamish Ivison</b> and Matthew E. Peters. 2022. Hyperdecoders: Instance-specific decoders for multi-task NLP. <i>Findings of EMNLP</i>.</span>
<br />
    
    <button class="btn btnId btnPub--abstract button-4" id="b_hyperdecoders-abstract" style="outline:none;">Abstract</button>
    

    <button class="btn btnId btnPub--BibTex button-4" id="b_hyperdecoders-bibtex" style="outline:none;">BibTeX</button>
    
    
    <a href="https://arxiv.org/abs/2203.08304"><button class="btn button-4 btnId btnPub--download" style="outline:none; position:relative;white-space: normal;">PDF</button></a>
    
    
    

    <div class="dropDownBibtex" id="hyperdecoders-bibtex" style="display:none;">
    <pre>@article{hyperdecoders,
  url = {https://arxiv.org/abs/2203.08304},
  author = {<b>Hamish Ivison</b> and Peters, Matthew E.},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Hyperdecoders: Instance-specific decoders for multi-task NLP},
  journal = {Findings of EMNLP},
  publisher = {arXiv},
  year = {2022}
}
</pre>
    </div>
    
    <div class="dropDownAbstract" id="hyperdecoders-abstract" style="display:none;">
    We investigate input-conditioned hypernetworks for multi-tasking in NLP, generating parameter-efficient adaptations for a decoder using a hypernetwork conditioned on the output of an encoder. This approach produces a unique decoder for every input instance, allowing the network a larger degree of flexibility than prior work that specializes the decoder for each task. We apply our method to sequence classification tasks, extractive QA, and summarisation and find that it surpasses previous parameter efficient fine-tuning methods and often outperforms fully finetuning the underlying model. An analysis of the embeddings used by our hypernetwork shows that they are sensitive to output label and type, suggesting that our approach better maps from encoder representations to output labels.
    </div></ul>
<ul><span id="localinterp">Siwen Luo*, <b>Hamish Ivison</b>*, Soyeon Caren Han, and Josiah Poon. 2021. Local Interpretations for Explainable Natural Language Processing:
               A Survey.</span>
<br />
    
    <button class="btn btnId btnPub--abstract button-4" id="b_localinterp-abstract" style="outline:none;">Abstract</button>
    

    <button class="btn btnId btnPub--BibTex button-4" id="b_localinterp-bibtex" style="outline:none;">BibTeX</button>
    
    
    <a href="https://arxiv.org/abs/2103.11072"><button class="btn button-4 btnId btnPub--download" style="outline:none; position:relative;white-space: normal;">PDF</button></a>
    
    
    

    <div class="dropDownBibtex" id="localinterp-bibtex" style="display:none;">
    <pre>@article{localinterp,
  author = {Luo*, Siwen and <b>Hamish Ivison</b>* and Han, Soyeon Caren and Poon, Josiah},
  title = {Local Interpretations for Explainable Natural Language Processing:
                 {A} Survey},
  year = {2021},
  url = {https://arxiv.org/abs/2103.11072},
  eprinttype = {arXiv},
  eprint = {2103.11072},
  timestamp = {Wed, 24 Mar 2021 15:50:40 +0100},
  biburl = {https://dblp.org/rec/journals/corr/abs-2103-11072.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
</pre>
    </div>
    
    <div class="dropDownAbstract" id="localinterp-abstract" style="display:none;">
    As the use of deep learning techniques has grown across various fields over the past decade, complaints about the opaqueness of the black-box models have increased, resulting in an increased focus on transparency in deep learning models. This work investigates various methods to improve the interpretability of deep neural networks for natural language processing (NLP) tasks, including machine translation and sentiment analysis. We provide a comprehensive discussion on the definition of the term interpretability and its various aspects at the beginning of this work. The methods collected and summarised in this survey are only associated with local interpretation and are divided into three categories: 1) explaining the model’s predictions through related input features; 2) explaining through natural language explanation; 3) probing the hidden states of models and word representations.

    </div></ul>
<ul><span id="thesis"><b>Hamish Ivison</b>. 2020. <i>Would you like fries with that? Modular Multi-hop Reasoning</i>. Honours Thesis, University of Sydney, November.</span>
<br />
    
    <button class="btn btnId btnPub--abstract button-4" id="b_thesis-abstract" style="outline:none;">Abstract</button>
    

    <button class="btn btnId btnPub--BibTex button-4" id="b_thesis-bibtex" style="outline:none;">BibTeX</button>
    
    
    <a href="/assets/static/thesis.pdf"><button class="btn button-4 btnId btnPub--download" style="outline:none; position:relative;white-space: normal;">PDF</button></a>
    
    
    

    <div class="dropDownBibtex" id="thesis-bibtex" style="display:none;">
    <pre>@thesis{thesis,
  author = {<b>Hamish Ivison</b>},
  title = {Would you like fries with that? Modular Multi-hop Reasoning},
  school = {University of Sydney},
  type = {Honours Thesis},
  year = {2020},
  month = nov,
  url = {/assets/static/thesis.pdf}
}
</pre>
    </div>
    
    <div class="dropDownAbstract" id="thesis-abstract" style="display:none;">
    In this work, we investigate an interpretable, modular approach to multi-hop question answering by adapting a popular visual question answering architecture, the MAC cell, to the task of multi-hop reading comprehension. In multi-hop reading comprehension, a model must answer questions by collating facts from multiple text sources. Our augmented MAC cell design outperforms existing modular approaches to multi-hop QA with less supervision and provides interpretable insights into its reasoning process. We then investigate integrating our cell with the highly popular BERT model and design a novel model which iteratively reads and retrieves documents in an interpretable fashion, allowing scalable and interpretable multi-hop question answering. Alongside this, we investigate the behaviour of generic BERT-based models on multi-hop QA and show that several existing approaches to multi-hop QA fail to significantly beat a naive BERT baseline. Our work shows the promise of MAC networks for multi-hop reasoning and outlines future paths for both MAC networks and multi-hop reasoning as a whole.
    </div></ul></div>


</div>
      </div>
    </main><footer class="site-footer h-card">
    <data class="u-url" href="/"></data>
  
    <div class="wrapper">
  
      <div class="footer-col-wrapper">
        <div class="footer-col">
        </div>
      </div>
      <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/hamishivi" title="hamishivi"><svg class="svg-icon "><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/hamish-ivison-6a17a1118" title="hamish-ivison-6a17a1118"><svg class="svg-icon "><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/hamishivi" title="hamishivi"><svg class="svg-icon "><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li><li>
    <a href="/feed.xml">
      <svg class="svg-icon">
        <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
      </svg>
    </a>
  </li>
</ul></div>
  
    </div>
  
  </footer></body>

</html>
