@article{hyperdecoders,
  url = {https://arxiv.org/abs/2203.08304},
  author = {<b>Hamish Ivison</b> and Peters, Matthew E.},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Hyperdecoders: Instance-specific decoders for multi-task NLP},
  journal = {Findings of EMNLP},
  publisher = {arXiv},
  year = {2022},
  abstract = {We investigate input-conditioned hypernetworks for multi-tasking in NLP, generating parameter-efficient adaptations for a decoder using a hypernetwork conditioned on the output of an encoder. This approach produces a unique decoder for every input instance, allowing the network a larger degree of flexibility than prior work that specializes the decoder for each task. We apply our method to sequence classification tasks, extractive QA, and summarisation and find that it surpasses previous parameter efficient fine-tuning methods and often outperforms fully finetuning the underlying model. An analysis of the embeddings used by our hypernetwork shows that they are sensitive to output label and type, suggesting that our approach better maps from encoder representations to output labels.}
}

@article{localinterp,
  author    = {Siwen Luo* and
               <b>Hamish Ivison</b>* and
               Soyeon Caren Han and
               Josiah Poon},
  title     = {Local Interpretations for Explainable Natural Language Processing:
               {A} Survey},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.11072},
  eprinttype = {arXiv},
  eprint    = {2103.11072},
  timestamp = {Wed, 24 Mar 2021 15:50:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-11072.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  abstract = {As the use of deep learning techniques has grown across various fields over the past decade, complaints about the opaqueness of the black-box models have increased, resulting in an increased focus on transparency in deep learning models. This work investigates various methods to improve the interpretability of deep neural networks for natural language processing (NLP) tasks, including machine translation and sentiment analysis. We provide a comprehensive discussion on the definition of the term interpretability and its various aspects at the beginning of this work. The methods collected and summarised in this survey are only associated with local interpretation and are divided into three categories: 1) explaining the model's predictions through related input features; 2) explaining through natural language explanation; 3) probing the hidden states of models and word representations.
}
}


