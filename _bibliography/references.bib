@article{deft,
  abstract = {Language models trained on massive prompted multitask datasets like T0 (Sanh et al., 2021) or FLAN (Wei et al., 2021a) can generalize to tasks unseen during training. We show that training on a carefully chosen subset of instances can outperform training on all available data on a variety of datasets. We assume access to a small number (250--1000) of unlabeled target task instances, select their nearest neighbors from a pool of multitask data, and use the retrieved data to train target task-specific models. Our method is more data-efficient than training a single multitask model, while still outperforming it by large margins. We evaluate across a diverse set of tasks not in the multitask pool we retrieve from, including those used to evaluate T0 and additional complex tasks including legal and scientific document QA. We retrieve small subsets of P3 (the collection of prompted datasets from which T0's training data was sampled) and finetune T5 models that outperform the 3-billion parameter variant of T0 (T0-3B) by 3--30% on 12 out of 14 evaluation datasets while using at most 2% of the data used to train T0-3B. These models also provide a better initialization than T0-3B for few-shot finetuning on target-task data, as shown by a 2--23% relative improvement over few-shot finetuned T0-3B models on 8 datasets. Our code is available at https://github.com/allenai/data-efficient-finetuning.},
  author = {<b>Hamish Ivison</b> and Noah A. Smith and Hannaneh Hajishirzi and Pradeep Dasigi},
  title = {Data-Efficient Finetuning Using Cross-Task Nearest Neighbors},
  journal = {arXiv (Under Review)},
  url = {https://arxiv.org/abs/2212.00196},
  year = {2022}
}

@article{hyperdecoders,
  url = {https://arxiv.org/abs/2203.08304},
  author = {<b>Hamish Ivison</b> and Peters, Matthew E.},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Hyperdecoders: Instance-specific decoders for multi-task NLP},
  journal = {Findings of EMNLP},
  publisher = {arXiv},
  year = {2022},
  abstract = {We investigate input-conditioned hypernetworks for multi-tasking in NLP, generating parameter-efficient adaptations for a decoder using a hypernetwork conditioned on the output of an encoder. This approach produces a unique decoder for every input instance, allowing the network a larger degree of flexibility than prior work that specializes the decoder for each task. We apply our method to sequence classification tasks, extractive QA, and summarisation and find that it surpasses previous parameter efficient fine-tuning methods and often outperforms fully finetuning the underlying model. An analysis of the embeddings used by our hypernetwork shows that they are sensitive to output label and type, suggesting that our approach better maps from encoder representations to output labels.}
}

@article{localinterp,
  author    = {Siwen Luo* and
               <b>Hamish Ivison</b>* and
               Soyeon Caren Han and
               Josiah Poon},
  title     = {Local Interpretations for Explainable Natural Language Processing:
               {A} Survey},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.11072},
  eprinttype = {arXiv},
  eprint    = {2103.11072},
  timestamp = {Wed, 24 Mar 2021 15:50:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-11072.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  abstract = {As the use of deep learning techniques has grown across various fields over the past decade, complaints about the opaqueness of the black-box models have increased, resulting in an increased focus on transparency in deep learning models. This work investigates various methods to improve the interpretability of deep neural networks for natural language processing (NLP) tasks, including machine translation and sentiment analysis. We provide a comprehensive discussion on the definition of the term interpretability and its various aspects at the beginning of this work. The methods collected and summarised in this survey are only associated with local interpretation and are divided into three categories: 1) explaining the model's predictions through related input features; 2) explaining through natural language explanation; 3) probing the hidden states of models and word representations.
}
}

@thesis{thesis,
  author       = {<b>Hamish Ivison</b>}, 
  title        = {Would you like fries with that? Modular Multi-hop Reasoning},
  school       = {University of Sydney},
  type={Honours Thesis},
  year         = 2020,
  month        = 11,
  url         = {/assets/static/thesis.pdf},
  abstract = {In this work, we investigate an interpretable, modular approach to multi-hop question answering by adapting a popular visual question answering architecture, the MAC cell, to the task of multi-hop reading comprehension. In multi-hop reading comprehension, a model must answer questions by collating facts from multiple text sources. Our augmented MAC cell design outperforms existing modular approaches to multi-hop QA with less supervision and provides interpretable insights into its reasoning process. We then investigate integrating our cell with the highly popular BERT model and design a novel model which iteratively reads and retrieves documents in an interpretable fashion, allowing scalable and interpretable multi-hop question answering. Alongside this, we investigate the behaviour of generic BERT-based models on multi-hop QA and show that several existing approaches to multi-hop QA fail to significantly beat a naive BERT baseline. Our work shows the promise of MAC networks for multi-hop reasoning and outlines future paths for both MAC networks and multi-hop reasoning as a whole.}
}


